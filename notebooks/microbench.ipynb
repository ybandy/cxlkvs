{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils.microbench_parser import MicrobenchParser\n",
    "from utils.plot import plot_with_models\n",
    "from utils.model import OpTimeModelBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWITCH_TIME = 50\n",
    "NUM_PREFETCHES = 12\n",
    "IO_COMPLETION_TIME = 140\n",
    "IO_SUBMISSION_TIME = 1690 - IO_COMPLETION_TIME\n",
    "PAUSE_TIME = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MicrobenchParser(['../microbench/log/dram.json',\n",
    "                       '../microbench/log/cxl.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = defaultdict(list)\n",
    "model = OpTimeModelBase(SWITCH_TIME, NUM_PREFETCHES)\n",
    "latency_list = mp.get_values('latency')\n",
    "for num_chases in mp.get_values('num_chases'):\n",
    "    # compute memory time offset from DRAM execution\n",
    "    min_memory_time = mp.get_values('memory_time')[0]\n",
    "    min_io_time_pre = mp.get_values('io_time_pre')[0]\n",
    "    min_io_time_post = mp.get_values('io_time_post')[0]\n",
    "    dram_time = 1e9 / mp.get_throughputs(latency_list, num_chases, min_memory_time, min_io_time_pre, min_io_time_post)[0]\n",
    "    model_time = model.reciprocal_throughput(num_chases, PAUSE_TIME * min_memory_time,\n",
    "                                             min_io_time_pre + IO_SUBMISSION_TIME, min_io_time_post + IO_COMPLETION_TIME, 0)\n",
    "    memory_time_offset = (dram_time - model_time) / num_chases\n",
    "\n",
    "    for memory_time in mp.get_values('memory_time'):\n",
    "        for io_time_pre in mp.get_values('io_time_pre'):\n",
    "            for io_time_post in mp.get_values('io_time_post'):\n",
    "                throughputs = mp.get_throughputs(latency_list, num_chases, memory_time, io_time_pre, io_time_post)\n",
    "                memory_time_nsec = PAUSE_TIME * memory_time + memory_time_offset\n",
    "                io_time_pre_ofs = io_time_pre + IO_SUBMISSION_TIME\n",
    "                io_time_post_ofs = io_time_post + IO_COMPLETION_TIME\n",
    "                title = '$M$ = %d, $T_{\\mathrm{mem}}$ = %d nsec, $T_{\\mathrm{IO}}^{\\mathrm{pre}}$ = %d nsec, $T_{\\mathrm{IO}}^{\\mathrm{post}}$ = %d nsec' % (num_chases, memory_time_nsec, io_time_pre_ofs, io_time_post_ofs)\n",
    "                name = 'microbench_M%d_mem%d_IOpre%d_IOpost%d' % (num_chases, memory_time_nsec, io_time_pre_ofs, io_time_post_ofs)\n",
    "                p = plot_with_models(latency_list, throughputs, num_chases, memory_time_nsec, io_time_pre_ofs, io_time_post_ofs,\n",
    "                                     SWITCH_TIME, NUM_PREFETCHES, 'Microbench', title, name)\n",
    "                for k, v in p.items():\n",
    "                    perfs[k] += v\n",
    "\n",
    "for k, v in perfs.items():\n",
    "    if(k != 'Microbench'):\n",
    "        ratios = list(np.array(v) / np.array(perfs['Microbench']))\n",
    "        print('%s: %.1f %% overestimate, %.1f %% underestimate' % (k, (max(ratios) - 1) * 100, (1 - min(ratios)) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
